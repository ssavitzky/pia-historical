<html>
<head>

<META name="keywords" contents="Billinghurst, Savage, Advanced Interface Projects">

<title>Adding Intelligence to the Interface - VRAIS '96</title>
</head>
<body>
<em>
&copy; 1996 IEEE. Personal use of this material is permitted. However, permission to
                              reprint/republish this material for advertising or promotional purposes or for creating new
                              collective works for resale or redistribution to servers or lists, or to reuse any copyrighted
                              component of this work in other works must be obtained from the IEEE.

<p>
This material is presented to ensure timely dissemination of scholarly and technical work.
                                        Copyright and all rights therein are retained by authors or by other copyright holders.
                                        All persons copying this information are expected to adhere to the terms and constraints
                                        invoked by each author's copyright. In most cases, these works may not be reposted
                                        without the explicit permission of the copyright holder. 

</em>

<h1 align="center">
Adding Intelligence to the Interface
</h1>
<b></b><p>
<b></b><i>M. Billinghurst, J. Savage</i><p>
<i></i>Human Interface Technology Laboratory<p>
University of Washington<p>
Box 352-142, Seattle,<p>
WA 98195<p>
{grof,savage}@hitl.washington.edu<p>
<p>
<em>
 Copyright (c) 1996 Institute of Electrical and Electronics Engineers.   
 Reprinted, with permission, from Proceedings of VRAIS '96.
</em>
<P>
<h3>Abstract</h3><i> </i><p>
<i>Virtual Reality has made computer interfaces more intuitive but not more
intelligent. This paper shows how a rule-based expert system can be combined
with multimodal input and natural language techniques to add intelligence to
the interface.  In this way virtual worlds can be built which recognize and
respond intelligently to user actions. Coupling voice, gesture, body position
and context together with an expert system creates an interface more powerful
than any of these elements alone; one which establishes a true two-way dialog
between human and machine.  To illustrate these techniques we present two
prototype systems that allow intuitive multimodal communication within
intelligent virtual environments.</i><p>
<p>
<H3>Introduction</H3>

<p>
Nearly thirty years ago Nicholas Negroponte introduced the idea of the
conversational computer[1], a machine that humans could interact with the same
way they do with each other on a daily basis; using voice, gaze, gesture and
body language.  Thirty years hence and the conversational computer is still
very much unreached, but with virtual reality it may be possible to simulate
such an interface. This has two immediate benefits.  Developing virtual worlds
around a conversational metaphor increases the responsiveness and intuitiveness
of these worlds - adding intelligence to the interface. Conversational virtual
interfaces give insights into the challenges that must be overcome before
conversational computers become commonplace in the real world.  <p>

<b></b>An ideal conversational interface allows the user to communicate using
an intuitive combination of input modalities, and should respond intelligently
to explicit and implicit conversational cues. This includes recognizing when
certain actions occur and making inferences about what should happen next. To
accomplish this the interface should have three components; multimodal input,
natural language understanding  and expert knowledge.  In this paper we
describe these components and show how a rule-based expert system can be used
to provide both low and high level understanding of events in the virtual
environment. We also present two prototypes illustrating how these methods may
be applied; an interface for multimodal communication with an intelligent
agent, and an intelligent sinus surgery simulator.<p>
 <p>
<h3>Multimodal Input</h3><p>

<b></b>Although humans communicate with each other through a range of different
modalities, human-machine interfaces rarely reflect this. Multimodal input is
particularly important in immersive virtual environments where the user does
not have access to traditional input devices. Voice, gesture and whole body
input have previously been shown to be extremely valuable in such settings
[2,3].<p>
<p>
Allowing combined voice and gestural input has three key advantages. First,
users prefer combined voice and gestural input when attempting graphics
manipulation. Hauptman and MacAvinney[4] used a simulated multimodal interface
to test subject response to three different input modes; gesture only, voice
only, and combined voice and gesture. Subjects overwhelmingly preferred
combined voice and gestural input due to the greater expressiveness possible.
When it was available, subjects used both speech and gesture 71% of the time
compared to voice (13%) or gesture (16%) only.<p>
<p>
Voice and gestural input compliment each other. Cohen [5,6] has shown how
natural language is suited for descriptive tasks, while gestures are ideal for
direct manipulation of objects. Similar results have also been found by
Ganapathy and Weiner [7] and Salisbury et. al.[8].<p>
<p>
Combining speech and gestural input improves recognition accuracy. By
integrating speech and gesture recognition, Bolt [9] discovered that neither
had to be perfect provided they converged on the intended meaning. For example,
if a user says "Make that square blue (pointing at a square)", and the speech
recognizer fails to identify "square", the sentence can still be understood by
considering the users gesture and the object pointed to.<p>
	    
<IMG SRC="figure1.gif" ALT="center" ALIGN=EVENTS IN  VIRTUAL  ENVIRONMENT>
<P align="center"><i>Figure 1: Events in the virtual environment add facts to the expert system
fact database, matching rules and causing an intelligent response.</i></p>

<H3>Natural Language Processing</H3>

<b></b><p>
Natural Language Processing (NLP) provides a theoretical framework for
interpreting and representing multimodal input. NLP is traditionally concerned
with understanding of written or spoken language, but may be modified to
incorporate other input modalities. In our approach natural language techniques
determine the representation scheme, while an expert system specifies how these
representations are manipulated in an intelligent way. To do this we draw on
established NL techniques such as Conceptual Dependencies and Scripts.<p>
<p>
A common theme in the natural language literature is that successful systems
use a combination of top down and bottom up processing, applying contextual
knowledge to assist in low-level parsing. For example, BORIS[10] is a program
which reads stories and responds to questions about them. Understanding begins
with bottom-up parsing of textual input which activates higher level semantic
structures. These structures are used to make predictions about possible
meaning which guides processing of subsequent input in a top-down manner.<p>
<p>
In a similar way, intelligent virtual environments should bring semantic and
contextual knowledge to bear as early as possible in the parsing process,
increasing recognition accuracy.  Also, by blending conceptual representation
of multimodal input at the lowest semantic level possible, the user can switch
input modalities at any time without loss of understanding by the system [11].
<p>
<b></b><p>
<H3>Expert System </H3>

<b></b><p>
An expert system can provide both low and high level understanding of the
events occurring in the virtual world. It does this by effectively integrating
voice and gestural input into a single semantic form which can be matched
against procedural and contextual knowledge to make inferences about user
actions.  In this way the system can recognize when specific actions occur in
the virtual environment and provide automatic intelligent feedback.  <p>
<p>
We use a rule-based system consisting of expert knowledge encoded in a set of
if-then production rules[12], such as: <i>if<b> FACT-0 </b> then
<b>DO-THIS</b></i>, where <b><i>FACT-0 </i></b> is the event necessary for rule
activation and <b><i>DO-THIS</i></b> the consequence of rule activation.  The
user's multimodal input and other events in the virtual environment generate
facts which are passed to the expert system fact database and matched against
the appropriate rules, causing an intelligent response, as shown in Figure 1.<p>
<p>
Rule-based expert systems have previously been applied to virtual environments
with the KARMA training system.[13]. In this case the rule-base was used to
determine when to show augmented  reality  training  images  to  the  user.
Stansfield et. al. [14] also use a rule-based expert system to monitor low
level events in a virtual environment training application. Our approach is
more general in that it supports both low level interpretation of multimodal
input and higher level understanding.<p>
<p>
Our expert system rule-base uses various levels of understanding and knowledge
representation, each based on those below it and each progressively more
knowledge intensive, as shown in Figure 2 overleaf. The lowest level, feature
based understanding, contains rules which identify and respond to low level
events such as body position. At the next level, semantic understanding, rules
identify and respond to isolated actions, such as the user giving a combined
voice and gestural command. Next there are rules which identify and respond to
different contexts (groups of related events). Finally, pragmatic understanding
identifies where in a sequence of tasks the user is, what they have already
done and what they should be doing next. Each of these levels of understanding
relies on those below it and contextual knowledge is applied at all levels to
ensure we have both top-down and bottom-up processing.  <p>

</body></html>

